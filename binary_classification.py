# -*- coding: utf-8 -*-
"""binary_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BzWIBpGIdDAjVZFXUPMYYZ2pBqQaknoV
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split as split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import itertools

from sklearn.datasets import make_circles as circ

#binary classification
n=1500

X,y=circ(n,
         noise=0.1,
         random_state=42)

X_train,X_test,y_train,y_test=split(X,y,test_size=0.3,random_state=42)

circles = pd.DataFrame({"X0":X[:, 0], "X1":X[:, 1], "label":y})
circles.head()

plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdYlBu);

X_train.shape,X_test.shape

tf.random.set_seed(42)

model=tf.keras.Sequential([
    tf.keras.layers.Dense(8,activation="relu"),
    tf.keras.layers.Dense(16,activation="relu"),
    tf.keras.layers.Dense(8,activation="relu"),
    tf.keras.layers.Dense(1,activation="sigmoid")
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])


# stops the training early if it stops seeing improvement
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

# initially the learning rate is kept high, but is decreased later to stablize the weights and biases
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='loss',
    factor=0.5,
    patience=3,
    verbose=1
)


# lr_schedule=tf.keras.callbacks.LearningRateScheduler(
#     lambda epoch :( 1e-4 * 10**(epoch/20))
# )



hist=model.fit(X_train,
               y_train,
               callbacks=[early_stopping,reduce_lr],
               epochs=150)

pd.DataFrame(hist.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs")

lrs= 1e-4 * (10**(tf.range(150)/20))
plt.figure(figsize=(10,7))
plt.semilogx(lrs,hist.history["loss"])
plt.xlabel("lr")
plt.ylabel("loss")

# # Ensure reproducibility
# tf.random.set_seed(42)

# # Define an improved model
# model = tf.keras.Sequential([
#     tf.keras.layers.Dense(64, activation="relu", kernel_initializer="he_normal"),
#     tf.keras.layers.BatchNormalization(),
#     tf.keras.layers.Dense(128, activation="relu", kernel_initializer="he_normal"),
#     tf.keras.layers.Dropout(0.3),  # Helps prevent overfitting
#     tf.keras.layers.Dense(64, activation="relu", kernel_initializer="he_normal"),
#     tf.keras.layers.BatchNormalization(),
#     tf.keras.layers.Dense(1, activation="sigmoid")  # Output for binary classification
# ])

# # Compile with a better optimizer
# model.compile(
#     loss=tf.keras.losses.BinaryCrossentropy(),
#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
#     metrics=["accuracy"]
# )

# # Train the model
# model.fit(X, y, epochs=100)

def plot_decision_boundary(model,X,y):
  x_min=X[:,0].min()-0.1
  x_max=X[:,0].max()+0.1
  y_min=X[:,1].min()-0.1
  y_max=X[:,1].max()+0.1
  xx,yy=np.meshgrid(np.linspace(x_min,x_max,100),
                    np.linspace(y_min,y_max,100))

  x_in=np.c_[xx.ravel(),yy.ravel()]

  y_pred=model.predict(x_in)

  if len(y_pred[0])>1:
    print("multiclass")
    y_pred=np.argmax(y_pred,axis=1).reshape(xx.shape)
  else:
    print("binary")
    y_pred=np.round(y_pred).reshape(xx.shape)


  plt.contourf(xx,yy,y_pred,cmap=plt.cm.RdYlBu,alpha=0.7)
  plt.scatter(X[:,0],X[:,1],c=y,s=40,cmap=plt.cm.RdYlBu)
  plt.xlim(xx.min(),xx.max())
  plt.ylim(yy.min(),yy.max())

plot_decision_boundary(model,X_test,y_test)

model.evaluate(X_test,y_test)

plt.figure(figsize=(12,7))
plt.subplot(1,2,1)
plot_decision_boundary(model,X_train,y_train)
plt.subplot(1,2,2)
plot_decision_boundary(model,X_test,y_test)

y_preds = (model.predict(X_test) >0.5).astype(int)
print(classification_report(y_test, y_preds))

y_preds = (model.predict(X_test) >0.5).astype(int)
print(confusion_matrix(y_test, y_preds))

figsize=(10,10)

cm=confusion_matrix(y_test,y_preds)
cm_norm=cm.astype("float")/cm.sum(axis=1)[:,np.newaxis]
n_classes=cm.shape[0]


fig,ax=plt.subplots(figsize=figsize)

cax=ax.matshow(cm,cmap=plt.cm.Blues)
fig.colorbar(cax)

classes=False

if classes:
  labels=classes
else:
  labels=np.arange(cm.shape[0])

ax.set(title="confusion matrix",
       xlabel="predicted",
       ylabel="true",
       xticks=np.arange(n_classes),
       yticks=np.arange(n_classes),
       xticklabels=labels,
       yticklabels=labels)

ax.xaxis.set_label_position('bottom')
ax.xaxis.tick_bottom()

ax.yaxis.label.set_size(20)
ax.xaxis.label.set_size(20)
ax.title.set_size(20)

threshold = (cm.max()+cm.min())/2

for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
  plt.text(j,i,f"{cm[i,j]}  ({cm_norm[i,j]*100:.1f}%)",
           horizontalalignment='center',
           color='white' if cm[i,j]>threshold else 'black',
           size=15
           )

cm_norm

